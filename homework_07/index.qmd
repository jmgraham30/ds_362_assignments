---
title: "DS 362 Homework 7"
subtitle: "SVMs Lab"
author: "Your Name"
format: 
  html:
    theme: pulse  
    toc: true
---
  
```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(ISLR2)
library(ggthemes)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))

```


This is Homework Assignment 7 for DS 362. You can view the source code for this assignment on GitHub: [view the source code](https://github.com/jmgraham30/ds_362_assignments/blob/main/homework_07/index.qmd).

For your amusement: What did the teacher put on top of their pizza? Graded cheese. 

## Instructions 

This homework corresponds to a lab assignment on support vector machines (SVMs). In working through this, you will learn how to use SVMs via the `tidymodels` framework. Please go through the lab, answering the questions or responding to the prompts as appropriate. 

1. Download the Quarto notebook for this assignment from the course learning management system

2. In the header, add your name as the author. 

3. Please complete and turn in this assignment as a Quarto notebook. 

## Lab

### Support Vector Classifier

Let us start by creating a synthetic data set. We will use some normally distributed data with an added offset to create 2 separate classes.

```{r}
set.seed(1)
sim_data <- tibble(
  x1 = rnorm(40),
  x2 = rnorm(40),
  y  = factor(rep(c(-1, 1), 20))
) %>%
  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),
         x2 = ifelse(y == 1, x2 + 1.5, x2))
```

Plotting it shows that we are having two slightly overlapping classes

```{r}
ggplot(sim_data, aes(x1, x2, color = y)) +
  geom_point() + 
  scale_color_colorblind()
```

We can then create a linear SVM specification by setting `degree = 1` in a polynomial SVM model. We furthermore set `scaled = FALSE` in `set_engine()` to have the engine scale the data for us. Alternatively, we scale using a recipe instead. 

:::{.callout-note}
`set_engine()` can be used to pass in additional arguments directly to the underlying engine. In this case, I'm passing in `scaled = FALSE` to `kernlab::ksvm()` which is the engine function.
:::

```{r}
svm_linear_spec <- svm_poly(degree = 1) %>%
  set_mode("classification") %>%
  set_engine("kernlab", scaled = FALSE)
```

**Exercise 1:** Explain each line of code in the previous code chunk.

Taking the specification, we can add a specific `cost` of $C = 10$ before fitting the model to the data. Using `set_args()` allows us to set the `cost` argument without modifying the model specification.

```{r}
svm_linear_fit <- svm_linear_spec %>% 
  set_args(cost = 10) %>%
  fit(y ~ ., data = sim_data)

svm_linear_fit
```

The `kernlab` models can be visualized using the `plot()` function if you load the `kernlab` package. 

```{r}
#| message: false
#| fig-alt: |
#|   Scatter chart of sim_data with x2 on the x-axis and x1 on the
#|   y-axis. A gradient going from red through white to blue,
#|   is overlaid. Blue values occur when both x1 and x2 sum to more
#|   than 2 and red values when x1 and x2 sum to less than 2.
#|   The gradient does not appear to seperate the two classes
#|   which is represented by shapes.
library(kernlab)
svm_linear_fit %>%
  extract_fit_engine() %>%
  plot()
```

what if we instead used a smaller value of the `cost` parameter?

**Exercise 2**: Add a new code chunk and modify the SVM model to have a cost value of $C=0.1$ instead.  What happens to the number of support vectors? Plot your results using the `plot()` function. How does the plot differ from before? 

#### Tuning a SVM

Let us set up a `tune_grid()` section to find the value of `cost` that leads to the highest accuracy for the SVM model.

```{r}
#| fig-alt: |
#|   Facetted connected scatter chart. x-axis shows different
#|   values of cost, and the y-axis show the performance metric
#|   value. The facets correspond to the two performance metrics
#|   accuracy and roc_auc. Both charts show a constant value for 
#|   all values of cost, expect for once where the accuracy skipes.
svm_linear_wf <- workflow() %>%
  add_model(svm_linear_spec %>% set_args(cost = tune())) %>%
  add_formula(y ~ .)

set.seed(1234)
sim_data_fold <- vfold_cv(sim_data, strata = y)

param_grid <- grid_regular(cost(), levels = 10)

tune_res <- tune_grid(
  svm_linear_wf, 
  resamples = sim_data_fold, 
  grid = param_grid
)

autoplot(tune_res)
```


**Exercise 3**: What impact does varying the cost parameter have on the accuracy of the model? What about the ROC AUC?

Let's use the `select_best()` function to find the value of `cost` that gives the best cross-validated accuracy. Finalize the workflow with `finalize_workflow()` and fit the new workflow on the data set.

```{r}
best_cost <- select_best(tune_res, metric = "accuracy")

svm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)

svm_linear_fit <- svm_linear_final %>% fit(sim_data)
```

We can now generate a different data set to act as the test data set. We will make sure that it is generated using the same model but with a different seed.

```{r}
set.seed(2)
sim_data_test <- tibble(
  x1 = rnorm(20),
  x2 = rnorm(20),
  y  = factor(rep(c(-1, 1), 10))
) %>%
  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),
         x2 = ifelse(y == 1, x2 + 1.5, x2))
```

Assessing the model on this testing data set shows us that the model still performs very well.

```{r}
augment(svm_linear_fit, new_data = sim_data_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

**Exercise 4**: Add another code chunk and use logistic regression to fit a model to the data. How does the accuracy compare to the SVM model? What about the ROC AUC? 

### Support Vector Machine

We will now see how to fit an SVM using a non-linear kernel. Let us start by generating some data, but this time generate with a non-linear class boundary.

```{r}
#| fig-alt: |
#|   Scatter plot of sim_data2. Data is in a oblong shape with
#|   points in the middle having color and both ends having
#|   another color.
set.seed(1)
sim_data2 <- tibble(
  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  y  = factor(rep(c(1, 2), c(150, 50)))
)

sim_data2 %>%
  ggplot(aes(x1, x2, color = y)) +
  geom_point() + 
  scale_color_colorblind()
```

We will try an SVM with a radial basis function. Such a kernel would allow us to capture the non-linearity in our data.

```{r}
svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

fitting the model

```{r}
svm_rbf_fit <- svm_rbf_spec %>%
  fit(y ~ ., data = sim_data2)
```

and plotting reveals that the model was able to separate the two classes, even though they were non-linearly separated.

```{r}
#| fig-alt: |
#|   Scatter chart of sim_data with x2 on the x-axis and x1 on the
#|   y-axis. A gradient going from red through white to blue,
#|   is overlaid. The grading is blue in the middle of the data
#|   and red at the edges, with a non-linear seperation between
#|   the colors.
svm_rbf_fit %>%
  extract_fit_engine() %>%
  plot()
```

Let's see how well this model generalizes to new data from the same generating process. 

```{r}
set.seed(2)
sim_data2_test <- tibble(
  x1 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  x2 = rnorm(200) + rep(c(2, -2, 0), c(100, 50, 50)),
  y  = factor(rep(c(1, 2), c(150, 50)))
)

augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

**Exercise 5**: Discuss how well the model performs on the testing data. 

### ROC Curves

ROC curves can easily be created using the `roc_curve()` function from the `yardstick` package loaded with `tidymodels`. We use this function much the same way as we have done using the `accuracy()` function, but the main difference is that we pass the predicted class probability instead of passing the predicted class.

```{r}
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_curve(truth = y, .pred_1)
```

This produces the different values of `specificity` and `sensitivity` for each threshold. We can get a quick visualization by passing the results of `roc_curve()` into `autoplot()`

```{r}
#| message: false
#| fig-alt: |
#|   A ROC curve plot. 1-specificity along the x-axis and
#|   sensitivity along the y-axis. A dotted line is drawn
#|   along the diagonal. The line quite closely follows
#|   the upper left side.
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_curve(truth = y, .pred_1) %>%
  autoplot()
```

As we have discussed before, a common performance metric is the area under the ROC curve. This can be done using the `roc_auc()` function (`_auc` stands for **a**rea **u**nder **c**urve).

```{r}
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_auc(truth = y, .pred_1)
```

**Exercise 6**: Create a new code chunk. In this code chunk, implement both a nearest neighbors model and a random forest model for classification on the same data we just used for the nonlinear SVM. Compare the ROC AUC for each model. Which model do you think performs the best? 

### Application to Gene Expression Data

We now examine the Khan data set, which consists of several tissue samples corresponding to four distinct types of small round blue cell tumors. For each tissue sample, gene expression measurements are available. The data set comes in the `Khan` list which we will wrangle a little bit to create two tibbles, 1 for the training data and 1 for the testing data.

```{r}
#| warning: false
Khan_train <- bind_cols(
  y = factor(Khan$ytrain),
  as_tibble(Khan$xtrain)
)

Khan_test <- bind_cols(
  y = factor(Khan$ytest),
  as_tibble(Khan$xtest)
)
```


looking at the dimensions of the training data reveals that we have 63 observations with 20308 gene expression measurements.

```{r}
dim(Khan_train)
```

There is a very large number of predictors compared to the number of rows. This indicates that a linear kernel will be preferable, as the added flexibility we would get from a polynomial or radial kernel is unnecessary.

```{r}
khan_fit <- svm_linear_spec %>%
  set_args(cost = 10) %>%
  fit(y ~ ., data = Khan_train)
```

Let us take a look at the training confusion matrix. And look, we get a perfect confusion matrix. We are getting this because the hyperplane was able to fully separate the classes.

```{r}
augment(khan_fit, new_data = Khan_train) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

But remember we don't measure the performance by how well it performs on the training data set. We measure the performance of a model on how well it performs on the testing data set, so let us look at the testing confusion matrix

```{r}
augment(khan_fit, new_data = Khan_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

**Exercise 7**: Discuss the performance of the model on the testing data.

**Exercise 8**: Now that you have some experience in using the `tidymodels` approach for fitting SMVs, use what you know to complete exercise 7 on page 401 of the ISLR textbook. 

