<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Your Name">

<title>DS 362 Homework 9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#instructions" id="toc-instructions" class="nav-link active" data-scroll-target="#instructions">Instructions</a></li>
  <li><a href="#part-i" id="toc-part-i" class="nav-link" data-scroll-target="#part-i">Part I</a></li>
  <li><a href="#part-ii" id="toc-part-ii" class="nav-link" data-scroll-target="#part-ii">Part II</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">DS 362 Homework 9</h1>
<p class="subtitle lead">Reinforcement Learning</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Your Name </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<p>This is Homework Assignment 9 for DS 362. You can view the source code for this assignment on GitHub: <a href="https://github.com/jmgraham30/ds_362_assignments/blob/main/homework_09/index.qmd">view the source code</a>.</p>
<p>For your amusement: Why did the robot win the dance contest? Because it was a dancing machine.</p>
<section id="instructions" class="level2">
<h2 class="anchored" data-anchor-id="instructions">Instructions</h2>
<p>This homework provides an introduction to <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> (RL) as a machine learning paradigm. In working through this, you will get an overview of RL and get some experience using the <code>ReinforcementLearning</code> package for R. Please go through the assignment, answering the questions or responding to the prompts as appropriate.</p>
<ol type="1">
<li><p>Download the Quarto notebook for this assignment from the course learning management system.</p></li>
<li><p>In the header, add your name as the author.</p></li>
<li><p>Please complete and turn in this assignment as a Quarto notebook.</p></li>
</ol>
</section>
<section id="part-i" class="level2">
<h2 class="anchored" data-anchor-id="part-i">Part I</h2>
<p>In this part, you are asked to watch portions of the first lecture for the Stanford course CS 234: <a href="http://web.stanford.edu/class/cs234/index.html">Reinforcement Learning</a> (RL) taught by Professor <a href="https://cs.stanford.edu/people/ebrun/">Emma Brunskill</a> and answer the provided questions.</p>
<p>Skim through the questions below and then watch from 00:01:38 to 00:16:03 and 00:22:04 to 1:04:00 of <a href="https://youtu.be/FgzM3zpZ55o?si=0F5Ug5ZCHVRzmMQ8">this video</a>. As you go through the video, answer the questions below.</p>
<ol type="1">
<li><p>What is a single sentence that describes reinforcement learning?</p></li>
<li><p>Why would an agent need to be intelligent?</p></li>
<li><p>What are three example applications of RL?</p></li>
<li><p>What are delayed consequences and some of the challenges they present to learning?</p></li>
<li><p>What is meant by “policy”?</p></li>
<li><p>Compare and contrast reinforcement learning, supervised learning, and unsupervised learning in the context of the four key concepts of RL: optimization, generalization, exploration, and delayed consequences.</p></li>
<li><p>Describe sequential decision making and give an example.</p></li>
<li><p>What is meant by “state” and what is the Markov assumption?</p></li>
<li><p>Why is the Markov assumption popular?</p></li>
<li><p>What are the key challenges in learning to make good sequential decisions?</p></li>
<li><p>Describe exploration versus exploitation and give an example. exploration, and delayed consequences.</p></li>
</ol>
</section>
<section id="part-ii" class="level2">
<h2 class="anchored" data-anchor-id="part-ii">Part II</h2>
<p>Here we give an introduction to the <code>ReinforcementLearning</code> package for R. This package allows one to perform model-free reinforcement in R. The implementation uses input data in the form of sample sequences consisting of states, actions and rewards. Based on such training examples, the package allows a reinforcement learning agent to learn an optimal policy that defines the best possible action in each state.</p>
<p>The <code>ReinforcementLearning</code> package utilizes different mechanisms for reinforcement learning, including Q-learning and experience replay. It thereby learns an optimal policy based on past experience in the form of sample sequences consisting of states, actions and rewards. Consequently, each training example consists of a state-transition tuple <span class="math inline">\((s_{i},a_{i},r_{i+1},s_{i+1})\)</span> as follows:</p>
<ul>
<li><p><span class="math inline">\(s_{i}\)</span> is the current environment state.</p></li>
<li><p><span class="math inline">\(a_{i}\)</span> denotes the selected action in the current state.</p></li>
<li><p><span class="math inline">\(r_{i+1}\)</span> specifies the immediate reward received after transitioning from the current state to the next state.</p></li>
<li><p><span class="math inline">\(s_{i+1}\)</span> refers to the next environment state.</p></li>
</ul>
<p>The training examples for reinforcement learning can (1) be collected from an external source and inserted into a tabular data structure, or (2) generated dynamically by querying a function that defines the behavior of the environment. In both cases, the corresponding input must follow the same tuple structure <span class="math inline">\((s_{i},a_{i},r_{i+1},s_{i+1})\)</span>. We detail both variants in the following.</p>
<p>We demonstrate the capabilities of the <code>ReinforcementLearning</code> package when using state-transition tuples from an external source without the need for modeling the dynamics of the environment.</p>
<p>The following example utilizes the <code>tictactoe</code> data set.</p>
<p><strong>Exercise 1:</strong> After installing and loading the <code>ReinforcementLearning</code> package, run the command <code>?tictactoe</code> in the R console and read the documentation for the <code>tictactoe</code> data set. What is contained in the data set?</p>
<p>The main function of <code>ReinforcementLearning</code> is <code>ReinforcementLearning()</code>. This function teaches a reinforcement learning agent using the previous input data. For this purpose, it requires the following arguments: (1) A data argument that must be a data frame object in which each row represents a state transition tuple <span class="math inline">\((s_{i},a_{i},r_{i+1},s_{i+1})\)</span>. (2) The column names of the individual tuple elements within data.</p>
<p>Several parameters can be provided to in order to customize the learning behavior of the agent.</p>
<ul>
<li><p><strong>alpha</strong> The learning rate, set between 0 and 1. Setting it to 0 means that the Q-values are never updated and, hence, nothing is learned. Setting a high value, such as 0.9, means that learning can occur quickly.</p></li>
<li><p><strong>gamma</strong> Discount factor, set between 0 and 1. Determines the importance of future rewards. A factor of 0 will render the agent short-sighted by only considering current rewards, while a factor approaching 1 will cause it to strive for a greater reward over the long run.</p></li>
<li><p><strong>epsilon</strong> Exploration parameter, set between 0 and 1. Defines the exploration mechanism in <span class="math inline">\(\epsilon\)</span>-greedy action selection. In this strategy, the agent explores the environment by selecting an action at random with probability <span class="math inline">\(\epsilon\)</span>. Alternatively, the agent exploits its current knowledge by choosing the optimal action with probability <span class="math inline">\(1-\epsilon\)</span>. This parameter is only required for sampling new experience based on an existing policy.</p></li>
<li><p><strong>iter</strong> Number of repeated learning iterations the agent passes through the training data set. Iter is an integer greater than 0. The default is set to 1 in which each state transition tuple is presented to the agent only once. Depending on the size of the training data, a higher number of repeated learning iterations can improve convergence but requires longer computation time. This parameter is passed directly to <code>ReinforcementLearning()</code>. The learning parameters alpha, gamma, and epsilon must be provided in an optional control object passed to the <code>ReinforcementLearning()</code> function.</p></li>
<li><p><strong>control</strong> A list of control parameters. The default is set to <code>list(alpha = 0.1, gamma = 0.9, epsilon = 0.1)</code>. This parameter is passed directly to <code>ReinforcementLearning()</code>.</p></li>
</ul>
<p>The following example utilizes the aforementioned dataset containing 406,541 game states of Tic-Tac-Toe to learn the optimal actions for each state of the board. All states are observed from the perspective of player <span class="math inline">\(X\)</span> who is also assumed to have played first. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. Reward for player <span class="math inline">\(X\)</span> is <span class="math inline">\(+1\)</span> for ‘win’, <span class="math inline">\(0\)</span> for ‘draw’, and <span class="math inline">\(-1\)</span> for ‘loss’.</p>
<p>The current state of the board is represented by a rowwise concatenation of the players’ marks in a 3x3 grid. For example, “……X.B” denotes a board state in which player <span class="math inline">\(X\)</span> has placed a mark in the first field of the third column whereas player B has placed a mark in the third field of the third column.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define reinforcement learning parameters</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">gamma =</span> <span class="fl">0.4</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform reinforcement learning</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">ReinforcementLearning</span>(tictactoe, <span class="at">s =</span> <span class="st">"State"</span>, <span class="at">a =</span> <span class="st">"Action"</span>, <span class="at">r =</span> <span class="st">"Reward"</span>, </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">s_new =</span> <span class="st">"NextState"</span>, <span class="at">iter =</span> <span class="dv">1</span>, <span class="at">control =</span> control)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate optimal policy</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>pol <span class="ot">&lt;-</span> <span class="fu">computePolicy</span>(model)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print policy</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pol)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BXBB.XXXB XXBB.XBXB X.BBX.X.B XX.BB..BX .BX.BBX.X BX.X.B.BX 
     "c5"      "c5"      "c1"      "c3"      "c8"      "c1" </code></pre>
</div>
</div>
<p><strong>Exercise 2:</strong> After running the previous code chunk, state what the function <code>computePolicy</code> does. Note that <code>c1</code> to <code>c9</code> denote the nine fields of the Tic-Tac-Toe board with <code>c1</code> corresponding to the first field of the first column and <code>c9</code> corresponding to the third field of the third column, where the indexing proceeds rowwise. The symbol <code>.</code> denotes an empty field.</p>
<p><strong>Exercise 3:</strong> Based on the output of the previous code chunk, state what the optimal action is for the agent to take when player <span class="math inline">\(X\)</span> places an initial mark on the center square of the board.</p>
<p><strong>Exercise 4:</strong> Based on the output of the previous code chunk, state what the optimal action is for the agent to take when player <span class="math inline">\(X\)</span> places an initial mark on the top left square of the board.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>